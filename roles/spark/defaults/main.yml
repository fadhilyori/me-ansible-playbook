---
# defaults file for ansible-role-spark
debug_mode: false
spark_version: "3.3.1"
spark_hadoop_version: "3"
scala_version: "2.13"
spark_mirror: "https://archive.apache.org/dist/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }}-scala{{ scala_version }}.tgz"
spark_download_check_checksum: false
spark_mirror_checksum: "sha512:https://archive.apache.org/dist/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }}-scala{{ scala_version }}.tgz.sha512"
spark_install_java: yes
spark_install_java_version: '17'
spark_download_over_internet: yes

# If you set spark_download_over_internet to no, you can place the downloaded file archive to files directory.
# spark_downloaded_archive_filename: spark-{{ spark_version }}-bin-hadoop{{ spark_hadoop_version }}.tgz

spark_src_dir: "/usr/local/src"
spark_usr_parent_dir: "/usr/local"
spark_conf_dir: "/etc/spark"
spark_lib_dir: "/var/lib/spark"
spark_usr_dir: "/usr/local/lib/spark"
spark_local_dirs: []
spark_local_dir_mode: "0755"
spark_tmp_dir: "/usr/local/spark/tmp"
spark_log_dir: "/var/log/spark"
spark_pid_dir: "/run/spark"
spark_env_file_path: "/etc/default/spark"

hadoop_conf_dir: /etc/hadoop

spark_user: "spark"
spark_user_groups: []
spark_user_shell: "/bin/nologin"

spark_symlinks_enabled: true

spark_env_extras: {
  SPARK_CONF_DIR: "{{ spark_conf_dir }}",
  SPARK_LOG_DIR: "{{ spark_log_dir }}",
  SPARK_PID_DIR: "{{ spark_pid_dir }}"
}
spark_defaults_extras: {
  spark.master: "spark://{{ groups['spark-master'][0] }}:7077",
  spark.driver.memory: 1g
}

# Example:
# pools:
#   - name: production
#     scheduling_mode: FIFO
#     weight: 2
#     min_share: 3

spark_pools: []

# Logger levels set to the official defaults
# Ref: https://github.com/apache/spark/blob/master/conf/log4j.properties.template
spark_log4j_loggers_level:
  # Set the default spark-shell log level to WARN. When running the spark-shell, the
  # log level for this class is used to overwrite the root logger's log level, so that
  # the user can have different defaults for the shell and regular Spark apps.
  - name: log4j.logger.org.apache.spark.repl.Main
    level: WARN
  # Settings to quiet third party logs that are too verbose
  - name: log4j.logger.org.spark_project.jetty
    level: WARN
  - name: log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle
    level: ERROR
  - name: log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper
    level: INFO
  - name: log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter
    level: INFO
  - name: log4j.logger.org.apache.parquet
    level: ERROR
  - name: log4j.logger.parquet
    level: ERROR
  # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
  - name: log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler
    level: FATAL
  - name: log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry
    level: ERROR

# Set everything to be logged to the console as default
spark_log4j_loggers_properties:
  - name: log4j.rootCategory
    value: INFO, console
  - name: log4j.appender.console
    value: org.apache.log4j.ConsoleAppender
  - name: log4j.appender.console.target
    value: System.err
  - name: log4j.appender.console.layout
    value: org.apache.log4j.PatternLayout
  - name: log4j.appender.console.layout.ConversionPattern
    value: "%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n"
  - name: log4j.rootCategory
    value: INFO, console
  - name: log4j.rootCategory
    value: INFO, console
  - name: log4j.rootCategory
    value: INFO, console
