---
- name: Ensure Spark directory exists
  file:
    path: "~/spark"
    state: directory
    mode: 0775

- name: Ensure Spark docker image dir exists
  file:
    path: "~/spark/docker_image"
    state: directory
    mode: 0775

- name: Copy docker image Spark related files to machine
  ansible.builtin.copy:
    src: "files/docker_images/{{ item }}"
    dest: "~/spark/docker_image/"
    mode: 0644
  with_items:
    - "mataelang_spark_3.3.1-scala2.13.tar"

- name: Load docker image
  ansible.builtin.shell: docker load -i {{ item }}
  args:
    chdir: ~/spark/docker_image
  with_items:
    - "mataelang_spark_3.3.1-scala2.13.tar"

- name: Configure docker-compose environment
  ansible.builtin.template: 
    src: ./spark/docker-compose.yml.j2
    dest: "~/spark/docker-compose.yml"
    mode: 0755

- name: Ensure Conf directory exists
  file:
    path: "~/spark/conf"
    state: directory
    mode: 0775

- name: Copy Kaspacore jar file to machine
  ansible.builtin.copy:
    src: "files/{{ item }}"
    dest: "~/spark/"
    mode: 0644
  with_items:
    - "kaspacore.jar"
    - "GeoLite2-City.mmdb"

- name: Ensure Kaspacore Checkpoint directory exists
  become: true
  file:
    path: "~/spark/kafka-checkpoint"
    state: directory
    mode: 0775
    owner: 185
    group: 185

- name: Down compose service
  ansible.builtin.shell: docker compose down -v
  args:
    chdir: ~/spark

- name: Load docker image
  ansible.builtin.shell: docker compose up -d
  args:
    chdir: ~/spark

- name: Pause for 1 minute to build app cache
  ansible.builtin.pause:
    minutes: 1

- name: Change owner of kafka-checkpoint directory
  become: true
  ansible.builtin.shell: chown -R 185:185 kafka-checkpoint
  args:
    chdir: ~/spark

- name: Execute spark-submit command
  ansible.builtin.shell: "{{ item }}"
  args:
    chdir: ~/spark
  with_items:
    - "docker compose exec -it spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --class org.mataelang.kaspacore.jobs.SensorEnrichDataStreamJob --total-executor-cores 1 --conf spark.submit.deployMode=cluster --conf spark.executor.cores=1 --conf spark.executor.memory=1g file:///opt/spark/jars/kaspacore.jar"
    - "docker compose exec -it spark-master /opt/spark/bin/spark-submit --master spark://spark-master:7077 --class org.mataelang.kaspacore.jobs.SensorAggregationStreamJob --total-executor-cores 5 --conf spark.submit.deployMode=cluster --conf spark.executor.cores=1 --conf spark.executor.memory=1g file:///opt/spark/jars/kaspacore.jar"